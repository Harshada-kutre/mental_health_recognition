{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#printing stopwords\n",
    "nltk.download('stopwords')\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id                                               text        class  \\\n",
      "0  302034  I made a grave mistake I donât remember the ...  non-suicide   \n",
      "1  302035  What series you like. I have watched all my fa...  non-suicide   \n",
      "2  302036  Guys I did it! I lost my virginity but it wasn...  non-suicide   \n",
      "3  302037  This guy like me or no? So, basically I have t...  non-suicide   \n",
      "4  302040  I have no hopeMy ex boyfriend cheated on me an...      suicide   \n",
      "\n",
      "   Unnamed: 3  Unnamed: 4  Unnamed: 5  Unnamed: 6  Unnamed: 7  \n",
      "0         NaN         NaN         NaN         NaN         NaN  \n",
      "1         NaN         NaN         NaN         NaN         NaN  \n",
      "2         NaN         NaN         NaN         NaN         NaN  \n",
      "3         NaN         NaN         NaN         NaN         NaN  \n",
      "4         NaN         NaN         NaN         NaN         NaN  \n"
     ]
    }
   ],
   "source": [
    "#data preprocessing\n",
    "mh_data=pd.read_csv('Sdata3.csv',encoding='unicode_escape')\n",
    "#printing first five lines\n",
    "print(mh_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id               8\n",
      "text            10\n",
      "class           16\n",
      "Unnamed: 3    4999\n",
      "Unnamed: 4    4999\n",
      "Unnamed: 5    4999\n",
      "Unnamed: 6    4999\n",
      "Unnamed: 7    4999\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#checking for missing values\n",
    "print(mh_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id            0\n",
      "text          0\n",
      "class         0\n",
      "Unnamed: 3    0\n",
      "Unnamed: 4    0\n",
      "Unnamed: 5    0\n",
      "Unnamed: 6    0\n",
      "Unnamed: 7    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#replacing missing values with null string\n",
    "mh_data=mh_data.fillna('')\n",
    "print(mh_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       non-suicide\n",
      "1       non-suicide\n",
      "2       non-suicide\n",
      "3       non-suicide\n",
      "4           suicide\n",
      "           ...     \n",
      "4994    non-suicide\n",
      "4995        suicide\n",
      "4996    non-suicide\n",
      "4997        suicide\n",
      "4998        suicide\n",
      "Name: class, Length: 4999, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#separating feature and target\n",
    "X=mh_data.drop(columns='class',axis=1)\n",
    "Y=mh_data['class']\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       I made a grave mistake I donât remember the ...\n",
      "1       What series you like. I have watched all my fa...\n",
      "2       Guys I did it! I lost my virginity but it wasn...\n",
      "3       This guy like me or no? So, basically I have t...\n",
      "4       I have no hopeMy ex boyfriend cheated on me an...\n",
      "                              ...                        \n",
      "4994    I found the news archive of when I first met m...\n",
      "4995    I wanted love, I wanted sex. I resuse to live ...\n",
      "4996    You ever just feel so god damn bored with peop...\n",
      "4997    Im an adoptee and I have a psychological abyss...\n",
      "4998    ConcernsI'm concerned that as a result of my s...\n",
      "Name: text, Length: 4999, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(mh_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       made grave mistak rememb post know someth lgbt...\n",
      "1       seri like watch favorit seri multipl time wond...\n",
      "2       guy lost virgin cool thought pp went soft minu...\n",
      "3       guy like basic friend keep give littl hint lik...\n",
      "4       hopemi ex boyfriend cheat gave genit herp hard...\n",
      "                              ...                        \n",
      "4994    found news archiv first met dad happi surviv t...\n",
      "4995    want love want sex resus live miss life greate...\n",
      "4996    ever feel god damn bore peopl talk like talk f...\n",
      "4997    im adopte psycholog abyss keep fill drugsand f...\n",
      "4998    concernsi concern result suicid wife get life ...\n",
      "Name: text, Length: 4999, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#stemming\n",
    "port_stem=PorterStemmer()\n",
    "def stemming(content):\n",
    "    stemmed_content=re.sub('[^a-zA-Z]',' ',content)\n",
    "    stemmed_content=stemmed_content.lower()\n",
    "    stemmed_content=stemmed_content.split()\n",
    "    stemmed_content=[port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]\n",
    "    stemmed_content=' '.join(stemmed_content)\n",
    "    return stemmed_content\n",
    "mh_data['text']=mh_data['text'].apply(stemming)\n",
    "print(mh_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['made grave mistak rememb post know someth lgbt commun made comment tri say someth along line straight like thought gay person whatev hell want came across homophob lost know care whether peopl get exactli word well'\n",
      " 'seri like watch favorit seri multipl time wonder guy favorit netflix mabi calm mabi watch brand new netflix seri'\n",
      " 'guy lost virgin cool thought pp went soft minut fuck' ...\n",
      " 'ever feel god damn bore peopl talk like talk famili peopl xbox parti everi singl day get fuck bore depress never feel lone ya know honestli wait person school abl go outsid talk peopl without health risk everywher look wish peopl talk'\n",
      " 'im adopte psycholog abyss keep fill drugsand feel disconnect feel one understand feel like belong peopl know wrong guy throw away reject like piec trash'\n",
      " 'concernsi concern result suicid wife get life insur pay mortgag cover set account die year worth money continu live decid next step']\n",
      "['non-suicide' 'non-suicide' 'non-suicide' ... 'non-suicide' 'suicide'\n",
      " 'suicide']\n"
     ]
    }
   ],
   "source": [
    "#X has features and Y has labes\n",
    "X=mh_data['text'].values\n",
    "Y=mh_data['class'].values\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4999,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4999,)\n"
     ]
    }
   ],
   "source": [
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 12844)\t0.1713405031480313\n",
      "  (0, 12701)\t0.19698814321810518\n",
      "  (0, 12680)\t0.16447898999780097\n",
      "  (0, 12652)\t0.12964274317463229\n",
      "  (0, 12544)\t0.07480519461328355\n",
      "  (0, 11811)\t0.0958454781529689\n",
      "  (0, 11497)\t0.11159907399719074\n",
      "  (0, 10900)\t0.19017143341871512\n",
      "  (0, 10541)\t0.2255195649519707\n",
      "  (0, 9880)\t0.10461945155746458\n",
      "  (0, 9457)\t0.16024980937085592\n",
      "  (0, 8773)\t0.12186842035404871\n",
      "  (0, 8510)\t0.11831795105396829\n",
      "  (0, 8464)\t0.09289403662880018\n",
      "  (0, 7332)\t0.1929422113172612\n",
      "  (0, 6907)\t0.25483521131337517\n",
      "  (0, 6813)\t0.14135133523790713\n",
      "  (0, 6665)\t0.19246591963031698\n",
      "  (0, 6647)\t0.07232615034064688\n",
      "  (0, 6590)\t0.26974259428276237\n",
      "  (0, 6382)\t0.16026095446051006\n",
      "  (0, 5403)\t0.2521699144440911\n",
      "  (0, 5222)\t0.15830205691678254\n",
      "  (0, 4896)\t0.26617909993750505\n",
      "  (0, 4683)\t0.08055508182679312\n",
      "  :\t:\n",
      "  (4997, 830)\t0.13759204208117484\n",
      "  (4997, 152)\t0.33788787154827343\n",
      "  (4997, 57)\t0.29852134261888685\n",
      "  (4998, 12979)\t0.11071541917275768\n",
      "  (4998, 12877)\t0.1913837575211378\n",
      "  (4998, 12758)\t0.2372901482127441\n",
      "  (4998, 11044)\t0.1209704087223339\n",
      "  (4998, 10824)\t0.22511205793049438\n",
      "  (4998, 10077)\t0.22034853660160178\n",
      "  (4998, 9547)\t0.23174335043041752\n",
      "  (4998, 8404)\t0.19459666485213833\n",
      "  (4998, 7737)\t0.17276012899109333\n",
      "  (4998, 7446)\t0.3449542598423693\n",
      "  (4998, 7398)\t0.17608367238652556\n",
      "  (4998, 6705)\t0.11808770654590307\n",
      "  (4998, 6609)\t0.10148796663770757\n",
      "  (4998, 5927)\t0.2553487134915726\n",
      "  (4998, 4683)\t0.09398627766824219\n",
      "  (4998, 2982)\t0.12506131149501507\n",
      "  (4998, 2756)\t0.18274753097458385\n",
      "  (4998, 2418)\t0.2553487134915726\n",
      "  (4998, 2315)\t0.19377098203697815\n",
      "  (4998, 2217)\t0.38492494432399094\n",
      "  (4998, 2215)\t0.24720940999348728\n",
      "  (4998, 81)\t0.2240076375980564\n"
     ]
    }
   ],
   "source": [
    "#converting textual data to numberical data\n",
    "vectorizer=TfidfVectorizer()\n",
    "vectorizer.fit(X)\n",
    "X=vectorizer.transform(X)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the labels values into numberics\n",
    "#from sklearn import preprocessing\n",
    "#label_encoder=preprocessing.LabelEncoder()\n",
    "#mh_data['class']=label_encoder.fit_transform(mh_data['class'])\n",
    "#mh_data['class'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting dataset into training and testing\n",
    "X_train,X_test,Y_train,Y_test= train_test_split(X,Y,test_size=0.2,stratify=Y,random_state=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries to fit the model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training logistic regression model\n",
    "model=LogisticRegression()\n",
    "model.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluating accuracy score on training data\n",
    "X_train_prediction=model.predict(X_train)\n",
    "training_data_accuracy=accuracy_score(X_train_prediction,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of the training data  0.9549887471867967\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score of the training data \",training_data_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating accuarcy score on the testing data\n",
    "X_test_prediction=model.predict(X_test)\n",
    "testing_data_accuracy=accuracy_score(X_test_prediction,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of the testing data  0.918\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score of the testing data \",testing_data_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['suicide']\n",
      "suicide\n"
     ]
    }
   ],
   "source": [
    "#Making a predictive system\n",
    "X_new=X_test[55]\n",
    "prediction=model.predict(X_new)\n",
    "print(prediction)\n",
    "print(Y_test[55])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
